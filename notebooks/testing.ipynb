{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e2699aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral, Negative, Positive\n"
     ]
    }
   ],
   "source": [
    "valid_sentiment = set(['Positive', 'Negative', 'Neutral'])\n",
    "print(', '.join(valid_sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2cb0e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import lightning as L\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score\n",
    "from torchmetrics.functional.classification import multiclass_accuracy, multiclass_f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import KeyedVectors\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "def load_embedding_matrix(file):\n",
    "    embedding_matrix = np.load(file)\n",
    "    return embedding_matrix\n",
    "\n",
    "def load_vocab(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    return vocab\n",
    "\n",
    "_embedding_matrix = load_embedding_matrix('/Users/kycdia/Documents/Project/situbondo coffee shop segmentation/app/artifacts/embedding_matrix.npy')\n",
    "_vocab = load_vocab('/Users/kycdia/Documents/Project/situbondo coffee shop segmentation/app/artifacts/vocab.pkl')\n",
    "\n",
    "class attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(attention, self).__init__()\n",
    "        self.attention_weights = nn.Linear(hidden_size * 2, hidden_size * 2, bias=False)\n",
    "        self.context_vector = nn.Linear(hidden_size * 2, 1, bias=False)\n",
    "\n",
    "    def forward(self, lstm_output):        \n",
    "        # Calculate the attention scores\n",
    "        scores = self.attention_weights(lstm_output)  # Shape: (batch_size, seq_len, hidden_size * 2)\n",
    "        scores = torch.tanh(scores)  # Apply non-linearity\n",
    "\n",
    "        # Calculate attention weights\n",
    "        scores = self.context_vector(scores)  # Shape: (batch_size, seq_len, 1)\n",
    "        scores = scores.squeeze(-1)  # Shape: (batch_size, seq_len)\n",
    "        \n",
    "        # Apply softmax to obtain attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)  # Shape: (batch_size, seq_len)\n",
    "\n",
    "        # Calculate the context vector as the weighted sum of the LSTM outputs\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), lstm_output)  # Shape: (batch_size, 1, hidden_size * 2)\n",
    "        context = context.squeeze(1)  # Shape: (batch_size, hidden_size * 2)\n",
    "\n",
    "        return context, attention_weights\n",
    "\n",
    "class biLSTM_sentiment(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_dim, n_layers, use_attention=True, bidirectional=True, dropout=0, freeze_embedding=False) -> None:\n",
    "        super(biLSTM_sentiment, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), freeze=freeze_embedding, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_matrix.shape[1], hidden_dim, num_layers=n_layers, bidirectional=bidirectional, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim * 2, hidden_dim//2, num_layers=n_layers, bidirectional=bidirectional, batch_first=True)\n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.attention = attention(hidden_dim)\n",
    "        self.use_attention = use_attention\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_dim = hidden_dim\n",
    "    def forward(self, text, text_lengths):\n",
    "        embedded = self.embedding(text)\n",
    "        text_lengths = text_lengths.to('cpu')\n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_out, (h, c) = self.lstm(packed_embedded)\n",
    "        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "\n",
    "        if self.use_attention:\n",
    "            attention_context, attention_weights = self.attention(lstm_out)\n",
    "            attention_context = attention_context.unsqueeze(1)\n",
    "        else:\n",
    "            attention_context = lstm_out\n",
    "\n",
    "        lstm2_out, _ = self.lstm2(attention_context)\n",
    "\n",
    "        if self.bidirectional:\n",
    "            h = torch.cat((lstm2_out[:, -1, :self.hidden_dim], lstm2_out[:, 0, :self.hidden_dim]), dim=1)\n",
    "        else:\n",
    "            h = h[-1,:,:]\n",
    "\n",
    "        out = self.fc(self.dropout(h))\n",
    "        return out, attention_weights\n",
    "\n",
    "class biLSTM_Attention(L.LightningModule):\n",
    "    def __init__(self, lr, num_classes, embedding_matrix, hidden_dim=256, dropout=0, use_attention=True, bidirectional=True,optim_decay=0, class_weight=None, isMPS = True, freeze_embedding=False):\n",
    "        super().__init__()\n",
    "        self.freeze_embedding = freeze_embedding\n",
    "        self.model = biLSTM_sentiment(embedding_matrix, hidden_dim, 3, 1, dropout=dropout, freeze_embedding=freeze_embedding, use_attention=use_attention, bidirectional=bidirectional)\n",
    "        self.isMPS = isMPS\n",
    "        if not isMPS:\n",
    "            self.accuracy = MulticlassAccuracy(num_classes)\n",
    "            self.F1Score = MulticlassF1Score(num_classes)\n",
    "        if class_weight is not None and len(class_weight) > 0:\n",
    "            self.criterion = nn.CrossEntropyLoss(weight=class_weight)\n",
    "        else:\n",
    "            self.criterion = nn.CrossEntropyLoss()\n",
    "        self.learning_rate = lr\n",
    "        self.num_classes = num_classes\n",
    "        self.optim_decay = optim_decay\n",
    "        self.use_attention = use_attention\n",
    "        self.save_hyperparameters()\n",
    "    def forward(self, text, text_lengths):\n",
    "        return self.model(text, text_lengths)\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        text, text_lengths, label = batch\n",
    "        if self.use_attention:\n",
    "            logits, weights = self.model(text, text_lengths)\n",
    "        else:\n",
    "            logits = self.model(text, text_lengths)\n",
    "        loss = self.criterion(logits, label)\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        if self.isMPS:\n",
    "            temp_preds = preds.to('cpu')\n",
    "            temp_label = label.to('cpu')\n",
    "            log_values = {'train_acc' : multiclass_accuracy(temp_preds, temp_label, num_classes=self.num_classes), 'train_F1Score' : multiclass_f1_score(temp_preds, temp_label, num_classes=self.num_classes), 'train_loss' : loss}\n",
    "            del temp_preds, temp_label\n",
    "        else:\n",
    "            log_values = {'train_acc' : self.accuracy(preds, label), 'train_F1Score' : self.F1Score(preds, label), 'train_loss' : loss}\n",
    "        self.log_dict(log_values, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        text, text_lengths, label = batch\n",
    "        if self.use_attention:\n",
    "            logits, weights = self.model(text, text_lengths)\n",
    "        else:\n",
    "            logits = self.model(text, text_lengths)\n",
    "        loss = self.criterion(logits, label)\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        if self.isMPS:\n",
    "            temp_preds = preds.to('cpu')\n",
    "            temp_label = label.to('cpu')\n",
    "            log_values = {'val_acc' : multiclass_accuracy(temp_preds, temp_label, num_classes=self.num_classes), 'val_F1Score' : multiclass_f1_score(temp_preds, temp_label, num_classes=self.num_classes), 'val_loss' : loss}\n",
    "            del temp_label, temp_preds\n",
    "        else:\n",
    "            log_values = {'val_acc' : self.accuracy(preds, label), 'val_F1Score' : self.F1Score(preds, label), 'val_loss' : loss}\n",
    "        self.log_dict(log_values, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr = self.learning_rate, weight_decay=self.optim_decay)\n",
    "        # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, mode='min', min_lr=0.0000001)\n",
    "        # return {\n",
    "        #     'optimizer' : optimizer,\n",
    "        #     'lr_scheduler' : {\n",
    "        #         'scheduler' : scheduler,\n",
    "        #         'monitor' : 'val_loss',\n",
    "        #         'frequency' : 1\n",
    "        #     }\n",
    "        # }\n",
    "        return optimizer\n",
    "    \n",
    "class inference_model:\n",
    "    def __init__(self, model, vocab, weights_path):\n",
    "        # self.model = model().load_from_checkpoint(weights_path).to('cpu').eval()\n",
    "        self.model = model\n",
    "        self.vocab = vocab\n",
    "        self.MAX_TOKEN = 256\n",
    "        self.class2idx = {\n",
    "            'Positive' : 0,\n",
    "            'Negative' : 1,\n",
    "            'Neutral' : 2\n",
    "        }\n",
    "        self.idx2class = {\n",
    "            0 : 'Positive',\n",
    "            1 : 'Negative',\n",
    "            2 : 'Neutral'\n",
    "        }\n",
    "    def __predict_prepare_data(self, sentence):\n",
    "        text = torch.LongTensor(self.__sentence2idx(sentence)).unsqueeze(0)\n",
    "        text_lengths = torch.tensor([len(self.__sentence2idx(sentence))])\n",
    "        return text, text_lengths\n",
    "    def __sentence2idx(self, sentence):\n",
    "        sentenceidx = []\n",
    "        for word in word_tokenize(sentence):\n",
    "            if re.match(r'^[a-zA-Z+$]', word):\n",
    "                word = word.lower()\n",
    "            if word in self.vocab:\n",
    "                sentenceidx.append(self.vocab[word])\n",
    "            else:\n",
    "                sentenceidx.append(0)\n",
    "        return sentenceidx\n",
    "    def predict(self, text):\n",
    "        prep_text, prep_len_text = self.__predict_prepare_data(text)\n",
    "        prep_len_text.to('cpu')\n",
    "        with torch.no_grad():\n",
    "            pred, context_weights = self.model(prep_text, prep_len_text)\n",
    "        pred = self.idx2class[int(torch.argmax(pred, 1))]\n",
    "        return pred, context_weights\n",
    "    def visualize_attention(self, text, title=True):\n",
    "        prediction, attention_weights = self.predict(text)\n",
    "        sentence_list = word_tokenize(text)\n",
    "        print(prediction)\n",
    "        plt.figure(figsize=(15,6))\n",
    "        sns.heatmap(attention_weights.cpu().numpy(), annot=False, cmap='Blues', xticklabels=sentence_list)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.xlabel('Tokens')\n",
    "        plt.ylabel('Attention Weights')\n",
    "        if title:\n",
    "            plt.title(text)\n",
    "        plt.show()\n",
    "\n",
    "mod = biLSTM_Attention.load_from_checkpoint(checkpoint_path='/Users/kycdia/Documents/Project/situbondo coffee shop segmentation/app/artifacts/new_epoch=8-step=720.ckpt', embedding_matrix=_embedding_matrix)\n",
    "inf_model = inference_model(mod, _vocab, '/Users/kycdia/Documents/Project/situbondo coffee shop segmentation/app/artifacts/new_epoch=8-step=720.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1cf0c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'review' : ['dem', 'nice', '', None]})\n",
    "mask = df['review'].apply(lambda x: isinstance(x, str) and x.strip() != '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c3c25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095d99c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
